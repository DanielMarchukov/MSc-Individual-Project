\section{Literature Review}
    \subsection{Overview}
        Broad overview of argumentation and various computational challenges associated with it is given in \autocite{Lippi2016ArgumentationMS}. Authors proceed to entertain the thoughts of what the breakthrough would mean on a societal level and the impact of it by solving these challenges. Moving on, the paper also describes different approaches that were previously experimented within the domains of argumentation and \gls{am}, mainly extracting arguments from unstructured text, which authors consider to be the core \gls{am} task. The core \gls{am} tasks are explained sufficiently for the reader to understand the differences between them. On the other hand, authors give only brief mentions to other \gls{am} tasks (opinionated claim analysis, premise verification, etc.), without giving a clear description of how they are different from each other. Also, the article does not provide any concrete evaluation of each methods, which makes it hard to gauge which approach has been the most successful so far or what the results were. Overall, the authors give a good overview of active research topics and different directions that the domain has taken.
        
    \subsection{Textual Entailment}
        \gls{te} is one of the most popular approaches to identify argument \textit{attack} and \textit{support} relationships. General introduction to using \gls{te} as a means for identifying how one argument stands in relation to the other is presented in \autocite{Cabrio2012NaturalLA, Cabrio2012CombiningTE}. Authors make a strong assumption that \textit{support} relation in \gls{baf} is the same as \textit{entailment} in \gls{te} and \textit{attack} in \gls{baf} is as \textit{contradiction} in \gls{te}. They test this theory on a set of arguments gathered from Debatepedia, an online debate forum where users express pros and cons on various topics. The language used at such discussions is quite different from loose and unstructured linguistics that can be seen in online social media corpus, which makes it hard to predict how the approach would do given a unpredictable environment. The system used for deciding if there exists a relation is called \gls{edits}, which states that the entailment relation probability of two arguments in a pair is greater the lower the number of changes need to be made between those two arguments, and vice versa. The chosen argument pair set is small, only 200 (100 used as a training set and 100 as test set), which is arguably insufficient to accurately assess the proposed approach, nevertheless, authors managed to accurately compute argument interrelations 75\% of the time, which is better than expected. This research was improved upon by exploring the \textit{support}-\textit{entailment} relationship, mostly by taking a step back on their previous assumption \autocite{Cabrio2013ANL, Cabrio2013DetectingBS}. The authors give an overview of semantic inferences in \gls{nlp}, in the sense that it is not always the case that an argument that supports another argument necessarily entails the latter argument in context. This was done by introducing a third labelling of argument pairs as \textit{null}, when such relation cannot be determined just from looking at those two arguments. Subsequently, same approach was applied to attacks, mainly by introducing 4 different types of complex attacks (\textit{supported}, \textit{mediated}, \textit{secondary}, \textit{extended}). The idea is similar, in that not all \textit{attacks} are \textit{contradictions}, thus making this case more subtle in general, requiring additional specification to identify attack relations. The results show that approximately 60\% of support argument pairs - \gls{te} holds; and for attacks - contradiction is present in about 70\% of argument pairs.
     
        Recently, the research in \gls{am} has taken a turn to social media, namely Twitter. Twitter is different from Debatepedia in that it often contains noisy, unstructured messages. This is mostly because of the specific restriction that Twitter imposes, being 140 character limit, which forces the user to express thoughts in short. Twitter is a good choice, because it provides open \gls{api} and it is possible to filter messages based on specific topics, that are defined with the use of a hashtag symbol (\#). Classification of tweets as argumentative or non-argumentative is presented in \autocite{Bosc2016DARTAD}. The methodology provides extensive description of how to treat different types of tweets, how to evaluate the substance of each tweet and shows relevant examples. A further study by the same authors put such dataset to practice \autocite{Bosc2016TweetiesSP}. The paper divides the work into 4 major steps that have to be taken from start to finish and discusses different approaches within each of those steps. The steps are as follows: \textit{1)} tweets need to be separated into two pools - either argumentative or non-argumentative; \textit{2)} argumentative tweets need to be further divided into separate groups by topic, and then, argument pairs have to be created; \textit{3)} argument relationship within the argument pairs have to be predicted as either \textit{attack} or \textit{support}; \textit{4)} argumentation graph has to be built. First and second tasks are trivial compared to the third one, as is evident based on views expressed in the paper. Different approaches were tested while tackling the third task, \gls{eop}, \gls{te}, neural sequence classifiers, though all with unsatisfying results, accuracies being less than 20\%.
     
    \subsection{Other Approaches}
        Here are briefly described other methods that were taken to extract arguments from unstructured texts:
        \begin{itemize}
            \item The \autocite{Cocarascu2017IdentifyingAA} paper uses Deep Learning and Long-Short Term Memory unidirectional and bidirectional networks to classify relations as either \textit{attack}, \textit{support} or \textit{neither support nor attack} and achieved an accuracy of 89.53\%. 
            
            \item In \autocite{Cocarascu2017MiningBA} the authors employ a methodology that tries to discover argument relations by grouping arguments into groups of similar topics. They showcase this by using an example containing hotel reviews, with reviews being grouped into 6 different topics. Also, the paper gives a brief description of how each topic could be given a strength score based on existing arguments.
            
            \item The \autocite{ApproxToTruth} paper analyses how a part of online comment network relates to the whole network in terms of acceptable conclusions drawn. In other words, by reading through an online debate, how similar conclusions can be drawn at that point in time compared to the overall consensus of the debate taking into equation all existing arguments. This is done by comparing grounded extensions of the two argumentation frameworks.
            
            \item The \autocite{Cocarascu2016DetectingDR} gives a \gls{ml} approach to detecting deceptive reviews by constructing topic independent \gls{aaf} and topic dependent \gls{baf}. Several algorithms (Random Forests, Logistic Regression, Na{\"i}ve Bayes) are used to evaluate the performances.
            
            \item In \autocite{Dusmanu2017ArgumentMO} supervised classification algorithms are used to detect arguments on Twitter. Authors also propose new avenues for research, namely facts recognition and source identification with regards to arguments.
            
            \item In \autocite{Lippi2015ContextIndependentCD}, the authors provide a context independent (topic independent) methodology for automatically extracting argument frameworks from unstructured text. They do this by splitting text into sentences and for each sentence building tree structures representing syntactical components of a sentence. The results are quite satisfactory with 74.6\%/68.4\% of precision/recall performances.
        \end{itemize}