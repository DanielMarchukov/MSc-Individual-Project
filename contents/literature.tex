\section{Literature Review} \label{literature}
    This chapter talks about existing literature relevant to this project. First and foremost, a brief overview is given on current state of the art research methodologies and results presented in a prominent article on the topic. Moving on, in \cref{textentail}, the discussion narrows down to specifically Textual Entailment field and review of past research results within this area. Then, an additional layer is added on top, and Textual Entailment is examined with a focus on analysing debates and interactions happening on social media platforms, and examines different consequences of working with natural and unstructured textual data (see \cref{socialmedia}). Lastly, the fourth subsection (see \cref{deeplearning}) talks about deep learning and how, when combined with Textual Entailment, it has seen success and popularity in application in academic environments.

    \subsection{Overview}
        A broad overview of argumentation and various computational challenges associated with it is given in \autocite{Lippi2016ArgumentationMS}. Authors proceed to entertain the thoughts of what the breakthrough would mean on a societal level and the impact of it by solving these challenges. Computing and evaluating natural language arguments is a hard task because of how unstructured natural language is in itself. There are no rules as to how long or short an argument has to be, sometimes even a single word disagreement can be considered an argument. On another note, even in biology it is still unclear out how human brains work and interpret meanings from words, sentences, paragraphs. This makes it so that \gls{ai} researches have to look at this problem from an empirical perspective, trying to guess which models, functions, parameters and their values lead to most accurate results. The future benefits of having an accurate argument mining and identification model with possibly an unlimited computational resources include being able to accurately track political debates, evaluating each argument and stances taken within the debate, presenting an objective and unbiased representation of current status (essentially, who is winning and by how much), increased fact checking and lie detection, all of which improve trust and reliability of social media. Overall, the examples here are just speculations and actual breakthroughs could be more substantial and cover more areas that are impacted.
        
        Moving on, the paper also describes different approaches that were previously experimented within the domains of argumentation and \gls{am}, mainly extracting arguments from unstructured text, which authors consider to be the core \gls{am} task. The \gls{am} tasks are explained sufficiently for the reader to understand the differences between them. The core tasks include argument detection in unstructured, natural language, topic independent text, then argument extraction, including the handling of nested arguments (arguments that are inside arguments), building of argument frameworks, analysing them and making appropriate decisions based on analysis results (e.g. identifying and removing duplicate forum threads automatically). 
        
        On the other hand, authors give only brief mentions to other \gls{am} tasks (opinionated claim analysis, premise verification, etc.), without giving a clear description of how they are different from each other. Also, the article does not provide any concrete evaluation of each methods, which makes it hard to gauge which approach has been the most successful so far or what the results were. Overall, the authors give a good overview of active research topics and different directions that the domain has taken.
        
    \subsection{Textual Entailment} \label{textentail}
        \gls{te} is one of the main corner stones, because the project deals with various arguments that when evaluated against other arguments, the degree of support or contraction varies a lot. Software developed in this project utilises \gls{te} to compute and assign a verdict to interrelationships between arguments.
    
        \gls{te} is a very popular approach used to identify argument \textit{attack} and \textit{support} relationships. General introduction to using \gls{te} as a means for identifying how one argument stands in relation to the other is presented in \autocite{Cabrio2012NaturalLA, Cabrio2012CombiningTE}. Authors make a strong assumption that \textit{support} relation in \gls{baf} is the same as \textit{entailment} in \gls{te} and \textit{attack} in \gls{baf} is as \textit{contradiction} in \gls{te}. They test this theory on a set of arguments gathered from Debatepedia, an online debate forum where users express pros and cons on various topics. The language used at such discussions is quite different from loose and unstructured linguistics that can be seen in online social media corpus, which makes it hard to predict how the approach would do given a unpredictable environment. The system used for deciding if there exists a relation is called \gls{edits}, which states that the entailment relation probability of two arguments in a pair is greater the lower the number of changes need to be made between those two arguments, and vice versa. The chosen argument pair set is small, only 200 (100 used as a training set and 100 as test set), which is arguably insufficient to accurately assess the proposed approach, nevertheless, authors managed to accurately compute argument interrelations 75\% of the time, which is better than expected. This research was improved upon by exploring the \textit{support}-\textit{entailment} relationship, mostly by taking a step back on their previous assumption \autocite{Cabrio2013ANL, Cabrio2013DetectingBS}. The authors give an overview of semantic inferences in \gls{nlp}, in the sense that it is not always the case that an argument that supports another argument necessarily entails the latter argument in context. This was done by introducing a third labelling of argument pairs as \textit{null}, when such relation cannot be determined just from looking at those two arguments. Subsequently, same approach was applied to attacks, mainly by introducing 4 different types of complex attacks (\textit{supported}, \textit{mediated}, \textit{secondary}, \textit{extended}). The idea is similar, in that not all \textit{attacks} are \textit{contradictions}, thus making this case more subtle in general, requiring additional specification to identify attack relations. The results show that approximately 60\% of support argument pairs - \gls{te} holds; and for attacks - contradiction is present in about 70\% of argument pairs.
        
    \subsection{Social Media Focus} \label{socialmedia}
        Social media is a growing phenomena in the past decade as the number of interactions happening simultaneously is unprecedented. This project tries to take advantage of that by utilising openly available \gls{api} access points to process and analyse public discussions, which can provide valuable insights into human psychology and decision making (this is excluding the possible advances within computer science and \gls{ai} fields) as opposed to carefully selected, structured and prepared data sets.
    
        Recently, the research in \gls{am} has taken a turn to social media, namely Twitter. Twitter is different from Debatepedia in that it often contains noisy, unstructured messages. This is mostly because of the specific restriction that Twitter imposes, being 140 character limit, which forces the user to express thoughts in short. Twitter is a good choice, because it provides open \gls{api} and it is possible to filter messages based on specific topics, that are defined with the use of a hashtag symbol (\#). Classification of tweets as argumentative or non-argumentative is presented in \autocite{Bosc2016DARTAD}. The methodology provides extensive description of how to treat different types of tweets, how to evaluate the substance of each tweet and shows relevant examples. A further study by the same authors put such dataset to practice \autocite{Bosc2016TweetiesSP}. The paper divides the work into 4 major steps that have to be taken from start to finish and discusses different approaches within each of those steps. The steps are as follows: \textit{1)} tweets need to be separated into two pools - either argumentative or non-argumentative; \textit{2)} argumentative tweets need to be further divided into separate groups by topic, and then, argument pairs have to be created; \textit{3)} argument relationship within the argument pairs have to be predicted as either \textit{attack} or \textit{support}; \textit{4)} argumentation graph has to be built. First and second tasks are trivial compared to the third one, as is evident based on views expressed in the paper. Different approaches were tested while tackling the third task, \gls{eop}, \gls{te}, neural sequence classifiers, though all with unsatisfying results, accuracies being less than 20\%.
    
    \subsection{Addition of Deep Learning Networks} \label{deeplearning}
        Deep Learning utilises concepts of biological neural connections and tries to simulate the information flow that happens inside the human brain. This makes this subfield of \gls{ml} an attractive choice when modelling solutions to various natural language problems, argument mining/detection being one of them.
        
        Deep Learning is used inside the project to train the classifier for determining different \gls{te} relations. In fact, the software incorporates \gls{lstm}, which is a special kind of neural network inside the overarching Deep Learning subject.
            
        Quite recently, \gls{lstm} has gained traction in research community when combined with Textual Entailment \autocite{Cocarascu2017IdentifyingAA}. \gls{lstm} simulates how human brains process sequential information (in other words, sentences), and remembers core keywords that contain most of the meaning in the sentence. This is mostly achieved by having a unidirectional/bi-directional \gls{lstm} networks to process the sentence forwards and backwards at the same time, taking into account how a word within a sentence provides context based on previously mentioned information or information that is yet to be mentioned. While such approach has an impressive 89.53\% accuracy rate, the way that the data set is structured raises a question on how much of it can be applied to real world conversations and social media. The main concern is that a single input can contain multiple sentences and on average is consisting of 50 words. In social media terms, these averages are quite large, as for example, Twitter imposes a 140 character limit per each twitter message, and fitting 50 words into 140 characters is impossible. Moreover, this problem is not limited to Twitter. In most other forums, Reddit and news articles' comment sections, it is only the case that the original post is usually lengthy and can consist of multiple paragraphs. Responses and debates usually consider of small, single sentence remarks that briefly describe the posters position, or express a short agreement/disagreement.